{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8yTCJ3GXWCm"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet ffmpeg-python git+https://github.com/openai/whisper.git > /dev/null\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGjPzuBTp8_-"
      },
      "outputs": [],
      "source": [
        "!pip install sounddevice"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "4Yt2Uh7QFDoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gowckBUW5lkr"
      },
      "outputs": [],
      "source": [
        "!apt-get install espeak\n",
        "!pip install pyttsx3\n",
        "pip install gtts tempfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "id": "-IttP3CNFVWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Receive Human Speech as Input"
      ],
      "metadata": {
        "id": "xUjxWhmfyQOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMa-yUk2XZlN"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import gradio as gr\n",
        "import whisper\n",
        "import openai\n",
        "from gtts import gTTS\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import tempfile\n",
        "import ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or9BARtFaD0K"
      },
      "outputs": [],
      "source": [
        "pip install pyttsx3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajcBNPwqaUCu"
      },
      "outputs": [],
      "source": [
        "!pip install gTTS\n",
        "\n",
        "from gtts import gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_Yg0dfaXqP0"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"OPEN_API_KEY\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is an HTML script that creates a button for recording audio. When the button is clicked, it starts recording audio from the user's device using the MediaRecorder API. Once the recording is stopped, the script converts the recorded audio into a base64-encoded string and displays a preview of the recorded audio on the web page."
      ],
      "metadata": {
        "id": "Y-2ANLzozXaa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9zoGfEFXvhd"
      },
      "outputs": [],
      "source": [
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"div\");\n",
        "var my_p = document.createElement(\"p\");\n",
        "var my_btn = document.createElement(\"button\");\n",
        "my_btn.style=\"font-size:1.5em;padding: 50px;\"\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "  };\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "    recordButton.onclick = ()=>{\n",
        "        toggleRecording();\n",
        "\n",
        "        sleep(2000).then(() => {\n",
        "            resolve(base64data.toString());\n",
        "            recordButton.innerText = \"Saved recording to audio.wav\"\n",
        "        });\n",
        "\n",
        "    }\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K40yqHgAX-iS"
      },
      "outputs": [],
      "source": [
        "#It retrieves the base64-encoded audio data from the HTML interface, decodes it, and then uses FFmpeg to convert it to a WAV file\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js('data')\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('audio.wav', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_ZoHRwcZLwb"
      },
      "outputs": [],
      "source": [
        "#It initiates the audio recording process\n",
        "def record_audio():\n",
        "    print(\"Recording...\")\n",
        "    get_audio()\n",
        "    print(\"Recording saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vAFnQEAZQiJ"
      },
      "outputs": [],
      "source": [
        "#Transcribes the audio content from a given audio path file\n",
        "def transcribe_audio(audio_path):\n",
        "    result = model.transcribe(audio_path)\n",
        "    transcription = result[\"text\"]\n",
        "    return transcription\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Get a response answer generated by GPT-3"
      ],
      "metadata": {
        "id": "JK3-gHR4ABGS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8w44Vd9QZhDc"
      },
      "outputs": [],
      "source": [
        "#Generate a completion for a given prompt\n",
        "def complete_prompt(prompt):\n",
        "    openai.api_key = \"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\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"babbage\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    gpt3_answer = response.choices[0].text.strip()\n",
        "    return gpt3_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Convert Response to Speech (TTS)"
      ],
      "metadata": {
        "id": "wF_WQSKH7BDc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_Umb5NrZ7Px"
      },
      "outputs": [],
      "source": [
        "#Converts Response to Speech\n",
        "def convert_to_speech(text):\n",
        "    tts = gTTS(text=text, lang=\"en\")\n",
        "    audio_file = \"output.mp3\"\n",
        "    tts.save(audio_file)\n",
        "    return audio_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Setting up Gradio Interface"
      ],
      "metadata": {
        "id": "JY8WYdgPkZUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "T34RrqYPfluN",
        "outputId": "2cbdbf1d-60b3-41a3-b439-606aa3baa711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "audio_input = gr.inputs.Audio(label=\"Record your voice\")\n",
        "audio_output = gr.outputs.Audio(type=\"numpy\", label=\"GPT-3 Response\")\n",
        "\n",
        "#Process function for the Gradio Interface\n",
        "def process_audio(audio):\n",
        "    transcription = transcribe_audio(audio)\n",
        "    response = complete_prompt(transcription)\n",
        "    tts_output = convert_to_speech(response)\n",
        "    return tts_output\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=audio_input,\n",
        "    outputs=audio_output,\n",
        "    title=\"OpenAI Whisper + GPT-3 Demo\",\n",
        "    description=\"Speak into the microphone and get a response from GPT-3\",\n",
        "    allow_screenshot=True\n",
        ")\n",
        "\n",
        "interface.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: ASR - GPT - TTS"
      ],
      "metadata": {
        "id": "lkJk68YSHuQB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFYYTTI8wmbi"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import tempfile\n",
        "import whisper\n",
        "import openai\n",
        "from gtts import gTTS\n",
        "import ffmpeg\n",
        "import sounddevice as sd\n",
        "import soundfile as sf\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "def get_audio():\n",
        "    \"\"\"\n",
        "    Gets audio from the microphone and saves it to a file.\n",
        "\n",
        "    Returns:\n",
        "      The path to the audio file.\n",
        "    \"\"\"\n",
        "    sample_rate = 16000  # Adjust this value if needed\n",
        "    duration = 10  # Adjust this value if needed\n",
        "\n",
        "    # Record audio from the microphone\n",
        "    recording = sd.rec(int(sample_rate * duration), samplerate=sample_rate, channels=1)\n",
        "    sd.wait()\n",
        "\n",
        "    # Save the audio to a file\n",
        "    audio_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "    audio_filename = audio_file.name\n",
        "    sf.write(audio_filename, recording, sample_rate)\n",
        "\n",
        "    return audio_filename\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    \"\"\"\n",
        "    Transcribes the audio file using Whisper.\n",
        "\n",
        "    Args:\n",
        "      audio_file: The path to the audio file.\n",
        "\n",
        "    Returns:\n",
        "      The transcribed text.\n",
        "    \"\"\"\n",
        "    with open(audio_file, \"rb\") as f:\n",
        "        audio_data = f.read()\n",
        "\n",
        "    response = whisper.transcribe(audio_data)\n",
        "    transcription = response[\"text\"]\n",
        "\n",
        "    return transcription\n",
        "\n",
        "def complete_prompt(prompt):\n",
        "    \"\"\"\n",
        "    Generates a response using GPT-3.\n",
        "\n",
        "    Args:\n",
        "      prompt: The prompt.\n",
        "\n",
        "    Returns:\n",
        "      The response generated by GPT-3.\n",
        "    \"\"\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    completion = response.choices[0].text.strip()\n",
        "    return completion\n",
        "\n",
        "def convert_to_speech(text):\n",
        "    \"\"\"\n",
        "    Converts text to speech using gTTS and saves it as an audio file.\n",
        "\n",
        "    Args:\n",
        "      text: The text to convert.\n",
        "\n",
        "    Returns:\n",
        "      The path to the audio file.\n",
        "    \"\"\"\n",
        "    tts = gTTS(text=text, lang=\"en\")\n",
        "    audio_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "    audio_filename = audio_file.name\n",
        "    tts.save(audio_filename)\n",
        "\n",
        "    return audio_filename\n",
        "\n",
        "def transcribe_and_generate(prompt, audio_file):\n",
        "    \"\"\"\n",
        "    Transcribes the audio file using Whisper and generates a response using GPT-3.\n",
        "\n",
        "    Args:\n",
        "      prompt: The prompt that was spoken.\n",
        "      audio_file: The path to the audio file.\n",
        "\n",
        "    Returns:\n",
        "      The response generated by GPT-3.\n",
        "    \"\"\"\n",
        "    # Transcribe the audio file.\n",
        "    transcription = transcribe_audio(audio_file)\n",
        "\n",
        "    # Generate a response using GPT-3.\n",
        "    completion = complete_prompt(prompt)\n",
        "    output_text = transcription + completion\n",
        "    output_file = convert_to_speech(output_text)\n",
        "    return output_file\n",
        "\n",
        "face = gr.Interface(\n",
        "    fn=transcribe_and_generate,\n",
        "    inputs=gr.inputs.Audio(\"microphone\"),\n",
        "    outputs=gr.outputs.Audio(type=\"filepath\"),\n",
        "    layout=\"vertical\",\n",
        "    title=\"EcoGenius\",\n",
        "    description=\"Speak or type a prompt and the EcoGenius will generate a response.\",\n",
        "    article=\"https://colab.research.google.com/drive/18k7NFznKpMUJz5L7YNcUkYjXxtka_R8K\"\n",
        ")\n",
        "\n",
        "face.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
